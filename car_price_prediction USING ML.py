# -*- coding: utf-8 -*-
"""car price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bEvlsJSPJNHgVsdzPW4VoJ5_vK2IErjL
"""

# importing dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn import metrics

# loading data
car_data = pd.read_csv('car data.csv')
car_data.head()

car_data.shape

# some basic info about data
car_data.info()

"""NO COLOUMN HAS NULL VALUES"""

# to check for null values
car_data.isnull().sum()

# checking the distribution of the categorical data
print(car_data.Fuel_Type.value_counts())
print(car_data.Transmission.value_counts())
print(car_data.Seller_Type.value_counts())

# converting categorical values we encode the data to numerical value for our model to understand
car_data.replace({'Fuel_Type':{'Petrol':0, 'Diesel':1,'CNG':2}},inplace = True)
car_data.replace({'Transmission':{'Manual':0, 'Automatic':1}},inplace = True)
car_data.replace({'Seller_Type':{'Dealer':0, 'Individual':1}},inplace = True)

car_data.head()

# splitting data into features and target column
X = car_data.drop(['Car_Name','Selling_Price'],axis = 1)
Y = car_data['Selling_Price']

X

Y

# splitting the data into train and test data
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.1,random_state = 2)

# training the model
# 1. linear regression
lin_reg_model = LinearRegression()

lin_reg_model.fit(X_train,Y_train)

# model evaluation on training data
training_data_prediction = lin_reg_model.predict(X_train)

# R2 error
error_score = metrics.r2_score(Y_train,training_data_prediction)
print('R2 error: ',error_score)

# plotting the original and predicted values
plt.scatter(Y_train,training_data_prediction)
plt.xlabel('Original Price')
plt.ylabel('Predicted Price')
plt.title('Actual Price vs Predicted Price')
plt.show()

test_data_prediction = lin_reg_model.predict(X_test)

# R2 error
error_score = metrics.r2_score(Y_test, test_data_prediction)
print('R2 error: ',error_score)

# plotting the original and predicted values
plt.scatter(Y_test,test_data_prediction)
plt.xlabel('Original Price')
plt.ylabel('Predicted Price')
plt.title('Actual Price vs Predicted Price')
plt.show()

# now we use the lasso regression model
lasso_reg_model = Lasso()

lasso_reg_model.fit(X_train,Y_train)

# model evaluation on training data
training_data_prediction = lasso_reg_model.predict(X_train)

# R2 error
error_score = metrics.r2_score(Y_train,training_data_prediction)
print('R2 error: ',error_score)

# plotting the original and predicted values
plt.scatter(Y_train,training_data_prediction)
plt.xlabel('Original Price')
plt.ylabel('Predicted Price')
plt.title('Actual Price vs Predicted Price')
plt.show()

test_data_prediction = lasso_reg_model.predict(X_test)

# R2 error
error_score = metrics.r2_score(Y_test, test_data_prediction)
print('R2 error: ',error_score)

# plotting the original and predicted values
plt.scatter(Y_test,test_data_prediction)
plt.xlabel('Original Price')
plt.ylabel('Predicted Price')
plt.title('Actual Price vs Predicted Price')
plt.show()

# 🚗 Enhanced Car Price Prediction with Advanced Features - Single Cell

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load data
df = pd.read_csv('car data.csv')

# Feature engineering
df['Car_Age'] = 2025 - df['Year']
df['Kms_per_Year'] = df['Kms_Driven'] / df['Car_Age'].replace(0, 1)
df['Kms_Driven_log'] = np.log1p(df['Kms_Driven'])

# Encode categorical variables
df.replace({'Fuel_Type': {'Petrol': 0, 'Diesel': 1, 'CNG': 2},
            'Seller_Type': {'Dealer': 0, 'Individual': 1},
            'Transmission': {'Manual': 0, 'Automatic': 1}}, inplace=True)

# Define features and target
features = ['Present_Price', 'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner',
            'Car_Age', 'Kms_per_Year', 'Kms_Driven_log']
X = df[features]
y = df['Selling_Price']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Model training
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation metrics
print("📈 R2 Score:", round(r2_score(y_test, y_pred), 3))
print("📉 MAE:", round(mean_absolute_error(y_test, y_pred), 3))
print("📊 RMSE:", round(np.sqrt(mean_squared_error(y_test, y_pred)), 3))

# Feature importance visualization
importances = pd.Series(model.feature_importances_, index=X.columns)
plt.figure(figsize=(8, 5))
importances.sort_values().plot(kind='barh', title='🔍 Feature Importances')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.show()

# Outlier detection (based on large residuals)
errors = y_test - y_pred
outliers = errors[np.abs(errors) > 2]
print(f"\n🚨 Outlier Alert: {len(outliers)} predictions deviated by > 2 Lakhs")

# Top undervalued & overvalued predictions
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Error': errors})
print("\n💸 Top 5 Most Undervalued Cars (Model underestimates value):")
print(comparison.sort_values(by='Error', ascending=False).head(5))

print("\n🧾 Top 5 Most Overvalued Cars (Model overestimates value):")
print(comparison.sort_values(by='Error').head(5))

# Optional: Simulated price suggestion (change inputs here)
sim_input = pd.DataFrame([{
    'Present_Price': 7.5,
    'Fuel_Type': 1,           # Diesel
    'Seller_Type': 0,         # Dealer
    'Transmission': 1,        # Automatic
    'Owner': 0,
    'Car_Age': 5,
    'Kms_per_Year': 12000,
    'Kms_Driven_log': np.log1p(60000)
}])

suggested_price = model.predict(sim_input)[0]
print(f"\n💡 Suggested Selling Price for Simulated Car: ₹{round(suggested_price, 2)} Lakhs")